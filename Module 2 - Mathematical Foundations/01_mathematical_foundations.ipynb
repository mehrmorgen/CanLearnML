{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147f8ca356ce4d7",
   "metadata": {},
   "source": [
    "# Module 2: Mathematical Foundations for Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the second module in our series on neural networks and language modeling! This notebook is designed for senior developers who are new to machine learning but have strong programming backgrounds.\n",
    "\n",
    "In this module, we'll explore the mathematical concepts that underpin machine learning algorithms. While you may have encountered some of these concepts before, we'll focus on their specific applications in ML and how they relate to the code you'll write.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Probability Distributions**: From discrete to continuous, and their role in ML\n",
    "- **Information Theory**: Entropy, cross-entropy, and KL divergence\n",
    "- **Loss Functions**: How to quantify model performance\n",
    "- **Optimization Basics**: Gradient descent and its variants\n",
    "\n",
    "Let's start by setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0d8008580cd603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:04:45.886415Z",
     "start_time": "2025-08-01T11:04:45.847108Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f98769a224237",
   "metadata": {},
   "source": [
    "## 1. Probability Distributions\n",
    "\n",
    "Probability distributions are fundamental to machine learning. They describe the likelihood of different outcomes and form the basis for many ML algorithms.\n",
    "\n",
    "### Discrete vs. Continuous Distributions\n",
    "\n",
    "- **Discrete distributions** deal with countable outcomes (like rolling a die)\n",
    "- **Continuous distributions** deal with uncountable outcomes (like height or weight)\n",
    "\n",
    "In language modeling, we often work with discrete distributions over characters or words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb61453d29ff3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:04:46.229533Z",
     "start_time": "2025-08-01T11:04:46.028102Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Let's load our names dataset again\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./names.txt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      3\u001B[39m     names = f.read().splitlines()\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Convert to lowercase\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/CanLearnML/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    338\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    339\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    340\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    341\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './names.txt'"
     ]
    }
   ],
   "source": [
    "# Let's load our names dataset again\n",
    "with open(\"../data/names.txt\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "# Convert to lowercase\n",
    "names = [name.lower() for name in names]\n",
    "\n",
    "# Look at first letter distribution (discrete)\n",
    "first_letters = [name[0] for name in names]\n",
    "letter_counts = {}\n",
    "\n",
    "for letter in first_letters:\n",
    "    if letter in letter_counts:\n",
    "        letter_counts[letter] += 1\n",
    "    else:\n",
    "        letter_counts[letter] = 1\n",
    "\n",
    "# Convert to probabilities\n",
    "total = len(first_letters)\n",
    "letter_probs = {k: v / total for k, v in letter_counts.items()}\n",
    "\n",
    "# Sort by letter\n",
    "letter_probs = dict(sorted(letter_probs.items()))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(letter_probs.keys(), letter_probs.values())\n",
    "plt.title(\"Probability Distribution of First Letters in Names\")\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cfa388e9d2fd6",
   "metadata": {},
   "source": [
    "### Probability Mass Function (PMF) vs. Probability Density Function (PDF)\n",
    "\n",
    "For discrete distributions, we use a **Probability Mass Function (PMF)** which gives the probability of each possible outcome.\n",
    "\n",
    "For continuous distributions, we use a **Probability Density Function (PDF)** which gives the relative likelihood of different outcomes. The probability of any specific value is 0, but the probability of a value falling within a range is the integral of the PDF over that range.\n",
    "\n",
    "Let's look at some common distributions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcbe88cbdad4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete: Binomial Distribution\n",
    "n, p = 10, 0.5  # number of trials, probability of success\n",
    "x = np.arange(0, n + 1)\n",
    "binomial = stats.binom.pmf(x, n, p)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, binomial)\n",
    "plt.title(f\"Binomial Distribution (n={n}, p={p})\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n",
    "\n",
    "# Continuous: Normal Distribution\n",
    "mu, sigma = 0, 1  # mean and standard deviation\n",
    "x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000)\n",
    "normal = stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, normal)\n",
    "plt.title(f\"Normal Distribution (μ={mu}, σ={sigma})\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.fill_between(x, normal, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8351706a63d8543",
   "metadata": {},
   "source": [
    "### Connection to Machine Learning\n",
    "\n",
    "In machine learning, we often:\n",
    "\n",
    "1. **Model data** using probability distributions\n",
    "2. **Estimate parameters** of these distributions from data\n",
    "3. **Make predictions** by sampling from or maximizing these distributions\n",
    "\n",
    "For example, in our character-level language model, we're modeling the probability distribution of the next character given the previous character(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344beaa387cc49e",
   "metadata": {},
   "source": [
    "## 2. Information Theory\n",
    "\n",
    "Information theory provides tools for quantifying information and uncertainty. It's particularly useful in machine learning for:\n",
    "\n",
    "- Measuring the information content of data\n",
    "- Quantifying the difference between probability distributions\n",
    "- Defining loss functions for training models\n",
    "\n",
    "### Entropy: Measuring Uncertainty\n",
    "\n",
    "**Entropy** measures the average amount of \"surprise\" or uncertainty in a probability distribution. It's defined as:\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} p(x) \\log p(x)$$\n",
    "\n",
    "Where:\n",
    "- $X$ is a random variable\n",
    "- $p(x)$ is the probability of outcome $x$\n",
    "- The logarithm is typically base 2 (bits) or base e (nats)\n",
    "\n",
    "Let's calculate the entropy of our first letter distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814caf5433c157b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs) -> float:\n",
    "    \"\"\"Calculate the entropy of a probability distribution.\"\"\"\n",
    "    return -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "\n",
    "\n",
    "# Calculate entropy of first letter distribution\n",
    "first_letter_entropy = entropy(letter_probs.values())\n",
    "print(f\"Entropy of first letter distribution: {first_letter_entropy:.4f} bits\")\n",
    "\n",
    "# Compare with uniform distribution\n",
    "uniform_probs = [1 / 26] * 26  # 26 letters in English alphabet\n",
    "uniform_entropy = entropy(uniform_probs)\n",
    "print(f\"Entropy of uniform distribution over 26 letters: {uniform_entropy:.4f} bits\")\n",
    "\n",
    "# Maximum possible entropy for 26 outcomes\n",
    "max_entropy = np.log2(26)\n",
    "print(f\"Maximum possible entropy for 26 outcomes: {max_entropy:.4f} bits\")\n",
    "\n",
    "# How much structure is in our distribution?\n",
    "print(f\"Information content (structure): {max_entropy - first_letter_entropy:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280506532c6b4fe",
   "metadata": {},
   "source": [
    "### Cross-Entropy: Measuring Prediction Quality\n",
    "\n",
    "**Cross-entropy** measures how well one probability distribution predicts samples from another. It's defined as:\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x} P(x) \\log Q(x)$$\n",
    "\n",
    "Where:\n",
    "- $P$ is the true distribution\n",
    "- $Q$ is our predicted distribution\n",
    "\n",
    "Cross-entropy is minimized when $P = Q$, and in that case, it equals the entropy of $P$.\n",
    "\n",
    "Let's calculate the cross-entropy between our first letter distribution and a uniform distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485108334c0053c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(true_probs: dict[str, float], pred_probs: dict[str, float]) -> float:\n",
    "    \"\"\"Calculate the cross-entropy between two probability distributions.\"\"\"\n",
    "    # Ensure both distributions have the same keys\n",
    "    all_keys = set(true_probs.keys()) | set(pred_probs.keys())\n",
    "\n",
    "    result = 0\n",
    "    for k in all_keys:\n",
    "        p = true_probs.get(k, 0)\n",
    "        q = pred_probs.get(k, 1e-10)  # Add small epsilon to avoid log(0)\n",
    "        if p > 0:\n",
    "            result -= p * np.log2(q)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Create a uniform distribution over the same letters\n",
    "uniform_dist = {k: 1 / len(letter_probs) for k in letter_probs.keys()}\n",
    "\n",
    "# Calculate cross-entropy\n",
    "ce = cross_entropy(letter_probs, uniform_dist)\n",
    "print(f\"Cross-entropy between true and uniform distributions: {ce:.4f} bits\")\n",
    "\n",
    "# Compare with entropy\n",
    "print(f\"Entropy of true distribution: {first_letter_entropy:.4f} bits\")\n",
    "print(f\"Difference (KL divergence): {ce - first_letter_entropy:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d1380f965f17c",
   "metadata": {},
   "source": [
    "### KL Divergence: Measuring Distribution Difference\n",
    "\n",
    "**Kullback-Leibler (KL) divergence** measures how one probability distribution differs from another. It's defined as:\n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)} = H(P, Q) - H(P)$$\n",
    "\n",
    "Where:\n",
    "- $P$ is the true distribution\n",
    "- $Q$ is our approximation\n",
    "- $H(P, Q)$ is the cross-entropy\n",
    "- $H(P)$ is the entropy of $P$\n",
    "\n",
    "KL divergence is always non-negative and equals zero only when $P = Q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564db797b429f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p: dict[str, float], q: dict[str, float]) -> float:\n",
    "    \"\"\"Calculate the KL divergence between two probability distributions.\"\"\"\n",
    "    return cross_entropy(p, q) - entropy(p.values())\n",
    "\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl = kl_divergence(letter_probs, uniform_dist)\n",
    "print(f\"KL divergence between true and uniform distributions: {kl:.4f} bits\")\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    letter_probs.keys(), letter_probs.values(), alpha=0.7, label=\"True Distribution\"\n",
    ")\n",
    "plt.bar(\n",
    "    uniform_dist.keys(), uniform_dist.values(), alpha=0.5, label=\"Uniform Distribution\"\n",
    ")\n",
    "plt.title(\"Comparison of Distributions\")\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13b44523dfca4b",
   "metadata": {},
   "source": [
    "### Connection to Machine Learning\n",
    "\n",
    "In machine learning:\n",
    "\n",
    "- **Entropy** helps us understand the inherent uncertainty in our data\n",
    "- **Cross-entropy** is commonly used as a loss function for classification problems\n",
    "- **KL divergence** is used in variational autoencoders and other generative models\n",
    "\n",
    "For our character-level language model, we'll use cross-entropy as our loss function to measure how well our model predicts the next character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76b78fb367cdd7",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "\n",
    "Loss functions quantify how well our model is performing. They measure the difference between our model's predictions and the true values, and we aim to minimize this difference during training.\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "#### Classification Problems\n",
    "\n",
    "- **Cross-Entropy Loss**: For multi-class classification\n",
    "- **Binary Cross-Entropy**: For binary classification\n",
    "- **Focal Loss**: For imbalanced classification\n",
    "\n",
    "#### Regression Problems\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Sensitive to outliers\n",
    "- **Mean Absolute Error (MAE)**: More robust to outliers\n",
    "- **Huber Loss**: Combines MSE and MAE\n",
    "\n",
    "Let's implement and visualize some of these loss functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb68a182c339549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of predicted probabilities\n",
    "y_pred = np.linspace(0.001, 0.999, 1000)\n",
    "\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "def binary_cross_entropy(y_true: float, y_pred: float) -> float:\n",
    "    return -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "\n",
    "\n",
    "# Mean Squared Error\n",
    "def mse(y_true: float, y_pred: float) -> float:\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "# Mean Absolute Error\n",
    "def mae(y_true: float, y_pred: float) -> float:\n",
    "    return np.abs(y_true - y_pred)\n",
    "\n",
    "\n",
    "# Plot loss functions for y_true = 1\n",
    "y_true = 1\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "mse_loss = mse(y_true, y_pred)\n",
    "mae_loss = mae(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_pred, bce_loss, label=\"Binary Cross-Entropy\")\n",
    "plt.plot(y_pred, mse_loss, label=\"MSE\")\n",
    "plt.plot(y_pred, mae_loss, label=\"MAE\")\n",
    "plt.title(f\"Loss Functions (y_true = {y_true})\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d66961e78a6c08",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood\n",
    "\n",
    "For our character-level language model, we'll use the **negative log-likelihood** as our loss function. This is equivalent to the cross-entropy loss when dealing with discrete distributions.\n",
    "\n",
    "The negative log-likelihood is defined as:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i} \\log P(y_i | x_i)$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the input (previous character(s))\n",
    "- $y_i$ is the true next character\n",
    "- $P(y_i | x_i)$ is the probability our model assigns to the true next character\n",
    "\n",
    "Let's see how this works with our bigram model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16092593072c4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple bigram model from our names dataset\n",
    "def build_bigram_model(names: list[str]) -> dict[str, dict[str, float]]:\n",
    "    # Add start and end tokens\n",
    "    names = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "    # Count bigrams\n",
    "    bigram_counts = {}\n",
    "    for name in names:\n",
    "        for c1, c2 in zip(name, name[1:], strict=False):\n",
    "            if c1 not in bigram_counts:\n",
    "                bigram_counts[c1] = {}\n",
    "            if c2 not in bigram_counts[c1]:\n",
    "                bigram_counts[c1][c2] = 0\n",
    "            bigram_counts[c1][c2] += 1\n",
    "\n",
    "    # Convert to probabilities\n",
    "    bigram_probs = {}\n",
    "    for c1 in bigram_counts:\n",
    "        total = sum(bigram_counts[c1].values())\n",
    "        bigram_probs[c1] = {\n",
    "            c2: count / total for c2, count in bigram_counts[c1].items()\n",
    "        }\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "\n",
    "# Build the model\n",
    "bigram_probs = build_bigram_model(names[:1000])  # Use a subset for speed\n",
    "\n",
    "# Calculate negative log-likelihood on a test name\n",
    "test_name = \"<emma>\"\n",
    "log_likelihood = 0\n",
    "\n",
    "for c1, c2 in zip(test_name, test_name[1:], strict=False):\n",
    "    # Get probability of c2 given c1\n",
    "    prob = bigram_probs.get(c1, {}).get(c2, 1e-10)  # Small epsilon to avoid log(0)\n",
    "    log_likelihood += np.log2(prob)\n",
    "    print(f\"P({c2}|{c1}) = {prob:.6f}, log2(prob) = {np.log2(prob):.6f}\")\n",
    "\n",
    "# Negative log-likelihood\n",
    "nll = -log_likelihood\n",
    "print(f\"\\nLog-likelihood: {log_likelihood:.6f}\")\n",
    "print(f\"Negative log-likelihood: {nll:.6f}\")\n",
    "\n",
    "# Average NLL per character (perplexity = 2^avg_nll)\n",
    "avg_nll = nll / (len(test_name) - 1)\n",
    "perplexity = 2**avg_nll\n",
    "print(f\"Average NLL per character: {avg_nll:.6f}\")\n",
    "print(f\"Perplexity: {perplexity:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250531475174efc",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "**Perplexity** is a common metric for evaluating language models. It's defined as:\n",
    "\n",
    "$$\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(y_i | x_i)} = 2^{\\text{avg\\_nll}}$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of predictions\n",
    "- $P(y_i | x_i)$ is the probability assigned to the true next character\n",
    "\n",
    "Perplexity can be interpreted as the weighted average number of choices the model is uncertain about at each step. Lower perplexity means better predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a99a23550c3e7",
   "metadata": {},
   "source": [
    "## 4. Optimization Basics\n",
    "\n",
    "Once we have a loss function, we need to optimize our model parameters to minimize this loss. The most common approach is **gradient descent** and its variants.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that works by:\n",
    "\n",
    "1. Computing the gradient (derivative) of the loss with respect to each parameter\n",
    "2. Updating each parameter in the direction of steepest descent\n",
    "3. Repeating until convergence\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta_t)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_t$ is the parameter at iteration $t$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla_\\theta \\mathcal{L}(\\theta_t)$ is the gradient of the loss with respect to $\\theta$\n",
    "\n",
    "Let's visualize gradient descent on a simple function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a8d1f6cbe9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 2D function\n",
    "def f(x: float, y: float) -> float:\n",
    "    return x**2 + y**2\n",
    "\n",
    "\n",
    "# Compute the gradient\n",
    "def grad_f(x: float, y: float) -> np.ndarray:\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "\n",
    "# Create a grid of points\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Perform gradient descent\n",
    "x0, y0 = 4, 4  # Starting point\n",
    "alpha = 0.1  # Learning rate\n",
    "max_iter = 20\n",
    "path = [(x0, y0)]\n",
    "\n",
    "for i in range(max_iter):\n",
    "    grad = grad_f(x0, y0)\n",
    "    x0 -= alpha * grad[0]\n",
    "    y0 -= alpha * grad[1]\n",
    "    path.append((x0, y0))\n",
    "\n",
    "# Plot the function and the gradient descent path\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, 50, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"f(x, y)\")\n",
    "plt.plot(*zip(*path, strict=False), \"ro-\", markersize=8)\n",
    "plt.title(\"Gradient Descent Optimization\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "iterations = range(len(path))\n",
    "loss_values = [f(x, y) for x, y in path]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, loss_values, \"bo-\")\n",
    "plt.title(\"Loss vs. Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f89b0334c9220b",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "The **learning rate** $\\alpha$ controls how big of a step we take in the direction of the gradient. It's a crucial hyperparameter:\n",
    "\n",
    "- If $\\alpha$ is too small, convergence will be slow\n",
    "- If $\\alpha$ is too large, we might overshoot the minimum or diverge\n",
    "\n",
    "Let's see the effect of different learning rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08785354693166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "max_iter = 20\n",
    "paths = {}\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    x0, y0 = 4, 4  # Starting point\n",
    "    path = [(x0, y0)]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x0, y0)\n",
    "        x0 -= alpha * grad[0]\n",
    "        y0 -= alpha * grad[1]\n",
    "        path.append((x0, y0))\n",
    "\n",
    "    paths[alpha] = path\n",
    "\n",
    "# Plot the function and the gradient descent paths\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, 50, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"f(x, y)\")\n",
    "\n",
    "for alpha, path in paths.items():\n",
    "    plt.plot(*zip(*path, strict=False), \"o-\", markersize=6, label=f\"α = {alpha}\")\n",
    "\n",
    "plt.title(\"Gradient Descent with Different Learning Rates\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations for each learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for alpha, path in paths.items():\n",
    "    iterations = range(len(path))\n",
    "    loss_values = [f(x, y) for x, y in path]\n",
    "    plt.plot(iterations, loss_values, \"o-\", label=f\"α = {alpha}\")\n",
    "\n",
    "plt.title(\"Loss vs. Iterations for Different Learning Rates\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa549f6263d025e",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In practice, computing the gradient over the entire dataset can be computationally expensive. **Stochastic Gradient Descent (SGD)** addresses this by:\n",
    "\n",
    "1. Computing the gradient on a small batch of data\n",
    "2. Updating the parameters based on this estimate\n",
    "3. Repeating with different batches\n",
    "\n",
    "This introduces noise in the optimization process but can lead to faster convergence and better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd223f7b3fa9ab66",
   "metadata": {},
   "source": [
    "### Variants of Gradient Descent\n",
    "\n",
    "Several variants of gradient descent have been developed to improve convergence:\n",
    "\n",
    "- **Momentum**: Adds a fraction of the previous update to the current one\n",
    "- **RMSprop**: Adapts the learning rate for each parameter based on the history of gradients\n",
    "- **Adam**: Combines momentum and RMSprop\n",
    "\n",
    "These optimizers are readily available in PyTorch and other ML frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed23ef762c81b5",
   "metadata": {},
   "source": [
    "## 5. Practical Application: Training a Simple Model\n",
    "\n",
    "Let's put these concepts together by training a simple model on our names dataset. We'll use PyTorch to implement a character-level language model that predicts the next character given the current one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d4e368bfe9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "with open(\"../../02 - Makemore/names.txt\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "# Convert to lowercase and add start/end tokens\n",
    "names = [\"<\" + name.lower() + \">\" for name in names]\n",
    "\n",
    "# Create vocabulary\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "stoi = dict(zip(chars, range(len(chars))))\n",
    "itos = dict(zip(range(len(chars)), chars))\n",
    "\n",
    "# Create training data\n",
    "xs = []  # Input characters\n",
    "ys = []  # Target characters (next character)\n",
    "\n",
    "for name in names:\n",
    "    for ch1, ch2 in zip(name, name[1:], strict=False):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f\"Vocabulary size: {len(chars)}\")\n",
    "print(f\"Number of examples: {len(xs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af37a3a05c2fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        # Initialize weights randomly\n",
    "        self.W = torch.randn(vocab_size, vocab_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        # One-hot encode the input\n",
    "        x_one_hot = torch.zeros(len(idx), self.vocab_size)\n",
    "        x_one_hot.scatter_(1, idx.unsqueeze(1), 1.0)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = x_one_hot @ self.W\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    def __call__(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Cross-entropy loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def update(self, lr: float = 0.1) -> None:\n",
    "        # Manual gradient descent\n",
    "        with torch.no_grad():\n",
    "            self.W -= lr * self.W.grad\n",
    "            self.W.grad = None\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = BigramLanguageModel(len(chars))\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Random batch\n",
    "    idx = torch.randint(0, len(xs), (batch_size,))\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(xs[idx])\n",
    "    loss = model.loss(logits, ys[idx])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    model.update(lr=0.1)\n",
    "\n",
    "    # Track loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c736b03f099802",
   "metadata": {},
   "source": [
    "### Generating Names\n",
    "\n",
    "Now that we've trained our model, let's use it to generate some new names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf81ba650248f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names\n",
    "def generate_name(model: BigramLanguageModel, max_len: int = 20) -> str:\n",
    "    out = []\n",
    "    idx = stoi[\"<\"]  # Start token\n",
    "\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        x_one_hot = torch.zeros(1, model.vocab_size)\n",
    "        x_one_hot[0, idx] = 1.0\n",
    "        logits = x_one_hot @ model.W\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Add to output\n",
    "        out.append(itos[idx])\n",
    "\n",
    "        # Check if we're done\n",
    "        if itos[idx] == \">\" or len(out) > max_len:\n",
    "            break\n",
    "\n",
    "    return \"\".join(out[:-1])  # Remove the end token\n",
    "\n",
    "\n",
    "# Generate 10 names\n",
    "for _ in range(10):\n",
    "    name = generate_name(model)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9483a2daf5d1f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we've covered the mathematical foundations of machine learning:\n",
    "\n",
    "1. **Probability Distributions**: The building blocks of statistical models\n",
    "2. **Information Theory**: Tools for measuring uncertainty and model quality\n",
    "3. **Loss Functions**: Ways to quantify model performance\n",
    "4. **Optimization**: Techniques for finding the best model parameters\n",
    "\n",
    "We've also applied these concepts to a practical problem: training a simple character-level language model to generate names.\n",
    "\n",
    "### Key Takeaways for Senior Developers\n",
    "\n",
    "- **Probability Theory**: ML models often represent and manipulate probability distributions\n",
    "- **Information Theory**: Concepts like entropy and cross-entropy provide principled ways to measure model performance\n",
    "- **Loss Functions**: Different problems require different loss functions\n",
    "- **Optimization**: Gradient-based methods are the workhorses of deep learning\n",
    "\n",
    "In the next module, we'll build on these foundations to explore the transition from manual counting to neural network-based approaches for language modeling."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
