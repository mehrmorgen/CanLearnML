{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91cc7b90e64fb1e5",
   "metadata": {},
   "source": [
    "# Module 3: From Manual to Automatic\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the third module in our series on neural networks and language modeling! This notebook demonstrates the progression from manual counting approaches to neural network-based approaches for language modeling.\n",
    "\n",
    "In this module, we'll implement both approaches side-by-side, allowing you to see how neural networks can learn the same patterns that we previously had to specify manually.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Manual Approach**: Implementing a bigram language model using explicit counting\n",
    "- **Neural Approach**: Building a neural network that learns the same patterns automatically\n",
    "- **Comparison**: Understanding the similarities and differences between these approaches\n",
    "- **Advantages**: Why neural networks are more powerful and flexible\n",
    "\n",
    "Let's start by setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040448a93513458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e154307af5d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97dffdb7b8f5fe",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing the Data\n",
    "\n",
    "We'll use the same names dataset as in previous modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d435e2d671eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the names dataset\n",
    "with open(\"../../02 - Makemore/names.txt\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "# Convert to lowercase\n",
    "names = [name.lower() for name in names]\n",
    "\n",
    "# Take a look at the first few names\n",
    "print(f\"First 10 names: {names[:10]}\")\n",
    "print(f\"Total number of names: {len(names)}\")\n",
    "\n",
    "# Add start and end tokens\n",
    "names_with_tokens = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "# Create vocabulary\n",
    "chars = sorted(list(set(\"\".join(names_with_tokens))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mappings between characters and indices\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e84ba3540e4c5a",
   "metadata": {},
   "source": [
    "## 2. Manual Approach: Bigram Counting\n",
    "\n",
    "First, let's implement the manual counting approach we saw in Module 1. We'll count all character bigrams in our dataset and use these counts to build a probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f087dbdc5055101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count bigrams\n",
    "def build_bigram_counts(names: list[str]) -> np.ndarray:\n",
    "    # Add start and end tokens\n",
    "    names = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "    # Initialize count matrix\n",
    "    counts = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    # Count bigrams\n",
    "    for name in names:\n",
    "        for c1, c2 in zip(name, name[1:], strict=False):\n",
    "            idx1 = char_to_idx[c1]\n",
    "            idx2 = char_to_idx[c2]\n",
    "            counts[idx1, idx2] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Build the bigram counts\n",
    "bigram_counts = build_bigram_counts(names)\n",
    "\n",
    "# Convert to probabilities\n",
    "# Add a small smoothing factor to avoid division by zero\n",
    "bigram_probs = (bigram_counts + 1) / (\n",
    "    bigram_counts.sum(axis=1, keepdims=True) + vocab_size\n",
    ")\n",
    "\n",
    "# Visualize the bigram probabilities\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(bigram_probs, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Bigram Probabilities\")\n",
    "plt.xlabel(\"Next Character\")\n",
    "plt.ylabel(\"Current Character\")\n",
    "plt.xticks(range(vocab_size), chars, rotation=90)\n",
    "plt.yticks(range(vocab_size), chars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fc04c2e359603",
   "metadata": {},
   "source": [
    "### Generating Names with the Manual Approach\n",
    "\n",
    "Now that we have our bigram probabilities, we can generate new names by sampling from these distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b06e5d949eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_manual(bigram_probs: np.ndarray, max_len: int = 20) -> str:\n",
    "    name = [\"<\"]  # Start with the start token\n",
    "\n",
    "    while True:\n",
    "        # Get the last character\n",
    "        last_char = name[-1]\n",
    "        last_idx = char_to_idx[last_char]\n",
    "\n",
    "        # Get the probability distribution for the next character\n",
    "        next_char_probs = bigram_probs[last_idx]\n",
    "\n",
    "        # Sample the next character\n",
    "        next_idx = np.random.choice(vocab_size, p=next_char_probs)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "\n",
    "        # Add to the name\n",
    "        name.append(next_char)\n",
    "\n",
    "        # Check if we're done\n",
    "        if next_char == \">\" or len(name) > max_len:\n",
    "            break\n",
    "\n",
    "    # Join and remove the tokens\n",
    "    return \"\".join(name[1:-1])\n",
    "\n",
    "\n",
    "# Generate 10 names\n",
    "print(\"Names generated using manual bigram approach:\")\n",
    "for _ in range(10):\n",
    "    print(generate_name_manual(bigram_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909953ce82819f4",
   "metadata": {},
   "source": [
    "## 3. Neural Approach: Learning Bigram Patterns\n",
    "\n",
    "Now, let's implement a neural network that learns the same bigram patterns automatically. We'll use a simple one-layer network with no hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516f57e75511f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "def prepare_training_data(names: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Add start and end tokens\n",
    "    names = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "    # Create input-output pairs\n",
    "    xs = []  # Input characters\n",
    "    ys = []  # Target characters (next character)\n",
    "\n",
    "    for name in names:\n",
    "        for c1, c2 in zip(name, name[1:], strict=False):\n",
    "            xs.append(char_to_idx[c1])\n",
    "            ys.append(char_to_idx[c2])\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "xs, ys = prepare_training_data(names)\n",
    "\n",
    "print(f\"Number of training examples: {len(xs)}\")\n",
    "print(\"First 5 input-output pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {idx_to_char[xs[i].item()]} â†’ {idx_to_char[ys[i].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d11ee24e1cfb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        # Initialize weights randomly\n",
    "        self.W = torch.randn(vocab_size, vocab_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        # One-hot encode the input\n",
    "        x_one_hot = torch.zeros(len(idx), self.vocab_size)\n",
    "        x_one_hot.scatter_(1, idx.unsqueeze(1), 1.0)\n",
    "\n",
    "        # Forward pass (equivalent to embedding lookup)\n",
    "        logits = x_one_hot @ self.W\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Cross-entropy loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def update(self, lr: float = 0.1) -> None:\n",
    "        # Manual gradient descent\n",
    "        with torch.no_grad():\n",
    "            self.W -= lr * self.W.grad\n",
    "            self.W.grad = None\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Random batch\n",
    "    idx = torch.randint(0, len(xs), (batch_size,))\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(xs[idx])\n",
    "    loss = model.loss(logits, ys[idx])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    model.update(lr=0.1)\n",
    "\n",
    "    # Track loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc83a9234977d3a",
   "metadata": {},
   "source": [
    "### Generating Names with the Neural Approach\n",
    "\n",
    "Now that we've trained our neural network, let's use it to generate new names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9733f4f73709459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_neural(model: BigramLanguageModel, max_len: int = 20) -> str:\n",
    "    out = []\n",
    "    idx = char_to_idx[\"<\"]  # Start token\n",
    "\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        x_one_hot = torch.zeros(1, model.vocab_size)\n",
    "        x_one_hot[0, idx] = 1.0\n",
    "        logits = x_one_hot @ model.W\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Add to output\n",
    "        out.append(idx_to_char[idx])\n",
    "\n",
    "        # Check if we're done\n",
    "        if idx_to_char[idx] == \">\" or len(out) > max_len:\n",
    "            break\n",
    "\n",
    "    return \"\".join(out[:-1])  # Remove the end token\n",
    "\n",
    "\n",
    "# Generate 10 names\n",
    "print(\"Names generated using neural network approach:\")\n",
    "for _ in range(10):\n",
    "    name = generate_name_neural(model)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dedde5edc9d2d",
   "metadata": {},
   "source": [
    "## 4. Comparing the Approaches\n",
    "\n",
    "Let's compare the learned weights of our neural network with the manually counted bigram probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187920a163cb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert neural network weights to probabilities\n",
    "neural_probs = torch.softmax(model.W, dim=1).detach().numpy()\n",
    "\n",
    "# Visualize the neural network probabilities\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(neural_probs, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Neural Network Learned Probabilities\")\n",
    "plt.xlabel(\"Next Character\")\n",
    "plt.ylabel(\"Current Character\")\n",
    "plt.xticks(range(vocab_size), chars, rotation=90)\n",
    "plt.yticks(range(vocab_size), chars)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the difference between manual and neural probabilities\n",
    "diff = np.abs(neural_probs - bigram_probs)\n",
    "\n",
    "# Visualize the difference\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(diff, cmap=\"Reds\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Difference Between Manual and Neural Probabilities\")\n",
    "plt.xlabel(\"Next Character\")\n",
    "plt.ylabel(\"Current Character\")\n",
    "plt.xticks(range(vocab_size), chars, rotation=90)\n",
    "plt.yticks(range(vocab_size), chars)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean absolute difference\n",
    "mean_diff = np.mean(diff)\n",
    "print(\n",
    "    f\"Mean absolute difference between manual and neural probabilities: {mean_diff:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6688e825564b125",
   "metadata": {},
   "source": [
    "## 5. Advantages of Neural Networks\n",
    "\n",
    "While our simple neural network learns essentially the same patterns as our manual counting approach, neural networks offer several advantages:\n",
    "\n",
    "1. **Scalability**: Neural networks can handle much larger vocabularies and context windows\n",
    "2. **Expressiveness**: They can learn more complex patterns beyond simple bigrams\n",
    "3. **Generalization**: They can generalize better to unseen examples\n",
    "4. **Feature Learning**: They can automatically learn useful features from the data\n",
    "\n",
    "Let's demonstrate this by extending our neural network to consider more context (trigrams instead of bigrams).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d24f23a2724ca",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we've seen how neural networks can learn the same patterns that we previously had to specify manually. We've implemented both approaches side-by-side and compared their results.\n",
    "\n",
    "Key takeaways:\n",
    "- Manual counting is explicit and interpretable but limited in scalability\n",
    "- Neural networks can learn the same patterns automatically from data\n",
    "- The neural approach can be extended to more complex models with minimal changes\n",
    "- Both approaches generate similar names, showing that the neural network has successfully learned the bigram patterns\n",
    "\n",
    "In the next module, we'll explore PyTorch fundamentals in more depth and build more sophisticated neural network architectures for language modeling."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
