{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93bec71c7326b2e",
   "metadata": {},
   "source": [
    "# Module 4: PyTorch Fundamentals\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the fourth module in our series on neural networks and language modeling! This notebook covers the essential PyTorch concepts needed for building more sophisticated neural network architectures.\n",
    "\n",
    "In this module, we'll explore PyTorch's core features and how they make deep learning development more efficient and intuitive. We'll build on the concepts from previous modules, but now using PyTorch's higher-level APIs.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Tensors**: PyTorch's fundamental data structure\n",
    "- **Autograd**: Automatic differentiation for building and training neural networks\n",
    "- **Neural Network API**: Building models with PyTorch's nn module\n",
    "- **Optimizers**: Using PyTorch's optimization algorithms\n",
    "- **Practical Example**: Building a more sophisticated language model\n",
    "\n",
    "Let's start by setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fd063e286432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131a359ab6e4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84080f0ea720a5c",
   "metadata": {},
   "source": [
    "## 1. Tensors: PyTorch's Fundamental Data Structure\n",
    "\n",
    "Tensors are multi-dimensional arrays similar to NumPy arrays but with additional features for deep learning. They can run on GPUs for accelerated computing and automatically track gradients for backpropagation.\n",
    "\n",
    "### Creating Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa575c8ccbe504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from Python lists\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"Tensor from list: {x}\")\n",
    "print(f\"Shape: {x.shape}, Dtype: {x.dtype}\")\n",
    "\n",
    "# Creating tensors with specific data types\n",
    "x_float = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "print(f\"Float tensor: {x_float}\")\n",
    "print(f\"Shape: {x_float.shape}, Dtype: {x_float.dtype}\")\n",
    "\n",
    "# Creating tensors with specific shapes\n",
    "zeros = torch.zeros(2, 3)  # 2x3 tensor of zeros\n",
    "ones = torch.ones(2, 3)  # 2x3 tensor of ones\n",
    "rand = torch.rand(2, 3)  # 2x3 tensor of random values from uniform distribution [0, 1)\n",
    "randn = torch.randn(\n",
    "    2, 3\n",
    ")  # 2x3 tensor of random values from normal distribution (mean=0, std=1)\n",
    "\n",
    "print(f\"Zeros:\\n{zeros}\")\n",
    "print(f\"Ones:\\n{ones}\")\n",
    "print(f\"Random uniform:\\n{rand}\")\n",
    "print(f\"Random normal:\\n{randn}\")\n",
    "\n",
    "# Creating tensors with specific devices\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = torch.tensor([1, 2, 3, 4], device=\"cuda\")\n",
    "    print(f\"GPU tensor: {x_gpu}\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea62cf94783468",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "PyTorch provides a rich set of operations for manipulating tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83211d1fa8e49134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"a - b = {a - b}\")\n",
    "print(f\"a * b = {a * b}\")  # Element-wise multiplication\n",
    "print(f\"a / b = {a / b}\")\n",
    "\n",
    "# Matrix operations\n",
    "m1 = torch.tensor([[1, 2], [3, 4]])\n",
    "m2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(f\"Matrix multiplication (m1 @ m2):\\n{m1 @ m2}\")\n",
    "print(f\"Element-wise multiplication (m1 * m2):\\n{m1 * m2}\")\n",
    "\n",
    "# Aggregation operations\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"Sum: {x.sum()}\")\n",
    "print(f\"Mean: {x.mean()}\")\n",
    "print(f\"Max: {x.max()}\")\n",
    "print(f\"Min: {x.min()}\")\n",
    "\n",
    "# Reshaping operations\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(f\"Original: {x}, shape: {x.shape}\")\n",
    "\n",
    "x_reshaped = x.reshape(2, 3)\n",
    "print(f\"Reshaped: \\n{x_reshaped}, shape: {x_reshaped.shape}\")\n",
    "\n",
    "x_view = x.view(\n",
    "    3, 2\n",
    ")  # View is similar to reshape but shares memory with the original tensor\n",
    "print(f\"View: \\n{x_view}, shape: {x_view.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eed59fe11f8407",
   "metadata": {},
   "source": [
    "### Converting Between NumPy and PyTorch\n",
    "\n",
    "PyTorch tensors can be easily converted to and from NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c251ff8c751cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array to PyTorch tensor\n",
    "np_array = np.array([1, 2, 3, 4, 5])\n",
    "tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "print(f\"PyTorch tensor: {tensor}\")\n",
    "\n",
    "# PyTorch tensor to NumPy array\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "np_array = tensor.numpy()\n",
    "\n",
    "print(f\"PyTorch tensor: {tensor}\")\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "\n",
    "# Note: When converting between NumPy and PyTorch, they share the same memory\n",
    "# if the tensor is on CPU and has a compatible data type\n",
    "np_array = np.array([1, 2, 3, 4, 5])\n",
    "tensor = torch.from_numpy(np_array)\n",
    "np_array[0] = 100  # Modify the NumPy array\n",
    "\n",
    "print(f\"Modified NumPy array: {np_array}\")\n",
    "print(f\"PyTorch tensor (shared memory): {tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fc1da954449a8",
   "metadata": {},
   "source": [
    "## 2. Autograd: Automatic Differentiation\n",
    "\n",
    "PyTorch's autograd system enables automatic computation of gradients, which is essential for training neural networks. It tracks operations on tensors and automatically computes gradients during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695cd359b6da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "z = x**2 + y**3\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"z = x^2 + y^3 = {z}\")\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(f\"dz/dx = {x.grad}\")  # Should be 2*x = 4\n",
    "print(f\"dz/dy = {y.grad}\")  # Should be 3*y^2 = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082810fa13417bc",
   "metadata": {},
   "source": [
    "### Gradient Accumulation and Zeroing\n",
    "\n",
    "By default, gradients accumulate in PyTorch. You need to zero them out before each backward pass if you're doing multiple iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ea81417ee1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First backward pass\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"First gradient: {x.grad}\")  # Should be 2*x = 4\n",
    "\n",
    "# Second backward pass (gradients accumulate!)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"Accumulated gradient: {x.grad}\")  # Should be 4 + 4 = 8\n",
    "\n",
    "# Zero out gradients\n",
    "x.grad.zero_()\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"After zeroing: {x.grad}\")  # Should be 4 again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ea2abd7739498",
   "metadata": {},
   "source": [
    "### Gradient Tracking Control\n",
    "\n",
    "Sometimes you want to disable gradient tracking to save memory or improve performance, especially during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146c78a127f2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torch.no_grad()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(f\"y computed with no_grad: {y}\")\n",
    "    print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "\n",
    "# Using .detach()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = y.detach()  # Creates a new tensor that doesn't require gradients\n",
    "\n",
    "print(f\"y: {y}, requires_grad: {y.requires_grad}\")\n",
    "print(f\"z: {z}, requires_grad: {z.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c75acc488ec26a",
   "metadata": {},
   "source": [
    "## 3. Neural Network API: torch.nn\n",
    "\n",
    "PyTorch's `nn` module provides high-level building blocks for creating neural networks. It includes layers, activation functions, loss functions, and more.\n",
    "\n",
    "### Building a Simple Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c12384823732eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Examine the parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c8a440e3ba8bb",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "Let's see how to perform a forward pass through our neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f14e7f27249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random input tensor\n",
    "batch_size = 3\n",
    "x = torch.randn(batch_size, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9094ced7b6ee538",
   "metadata": {},
   "source": [
    "### Common Neural Network Layers\n",
    "\n",
    "PyTorch provides a wide range of layers for building neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a663c96e0af3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear (Fully Connected) Layer\n",
    "linear = nn.Linear(10, 5)\n",
    "x = torch.randn(3, 10)\n",
    "output = linear(x)\n",
    "print(f\"Linear layer output shape: {output.shape}\")\n",
    "\n",
    "# Convolutional Layer\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "x = torch.randn(1, 3, 28, 28)  # (batch_size, channels, height, width)\n",
    "output = conv(x)\n",
    "print(f\"Conv2d layer output shape: {output.shape}\")\n",
    "\n",
    "# Recurrent Layer\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n",
    "x = torch.randn(3, 5, 10)  # (batch_size, sequence_length, input_size)\n",
    "output, hidden = rnn(x)\n",
    "print(f\"RNN output shape: {output.shape}\")\n",
    "print(f\"RNN hidden state shape: {hidden.shape}\")\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = nn.Embedding(num_embeddings=100, embedding_dim=8)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "output = embedding(x)\n",
    "print(f\"Embedding layer output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7581c0a5cfa75",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0c07b5af5472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample tensor\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Apply different activation functions\n",
    "relu = F.relu(x)\n",
    "sigmoid = torch.sigmoid(x)\n",
    "tanh = torch.tanh(x)\n",
    "softmax = F.softmax(x, dim=0)  # Softmax along the first dimension\n",
    "\n",
    "# Plot the activation functions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x.numpy(), relu.numpy())\n",
    "plt.title(\"ReLU\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x.numpy(), sigmoid.numpy())\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x.numpy(), tanh.numpy())\n",
    "plt.title(\"Tanh\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x.numpy(), softmax.numpy())\n",
    "plt.title(\"Softmax\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98d8c629f299d9",
   "metadata": {},
   "source": [
    "## 4. Optimizers: Training Neural Networks\n",
    "\n",
    "PyTorch provides various optimization algorithms for training neural networks. These optimizers update the model parameters based on the computed gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fa7f82157c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(5):\n",
    "    # Forward pass\n",
    "    x = torch.randn(5, 10)\n",
    "    y_true = torch.randn(5, 1)\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(y_pred, y_true)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858d3ad7dc0da31",
   "metadata": {},
   "source": [
    "### Common Optimizers\n",
    "\n",
    "PyTorch provides several optimization algorithms, each with its own characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff85fc4097420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Create different optimizers\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "sgd_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "adam = optim.Adam(model.parameters(), lr=0.01)\n",
    "rmsprop = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Available optimizers:\")\n",
    "print(f\"SGD: {sgd}\")\n",
    "print(f\"SGD with momentum: {sgd_momentum}\")\n",
    "print(f\"Adam: {adam}\")\n",
    "print(f\"RMSprop: {rmsprop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743115d17d31d7f",
   "metadata": {},
   "source": [
    "### Learning Rate Schedulers\n",
    "\n",
    "Learning rate schedulers adjust the learning rate during training, which can help improve convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827396c6d914c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\n",
    "\n",
    "# Create a model and optimizer\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Create different schedulers\n",
    "step_scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "exp_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2)\n",
    "\n",
    "# Simulate training with StepLR\n",
    "print(\"Training with StepLR:\")\n",
    "for epoch in range(5):\n",
    "    # Training code would go here\n",
    "    print(f\"Epoch {epoch}, LR: {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "    step_scheduler.step()\n",
    "\n",
    "# Reset optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Simulate training with ExponentialLR\n",
    "print(\"\\nTraining with ExponentialLR:\")\n",
    "for epoch in range(5):\n",
    "    # Training code would go here\n",
    "    print(f\"Epoch {epoch}, LR: {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "    exp_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc55cdcebebb00b",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Building a Language Model\n",
    "\n",
    "Let's apply what we've learned to build a more sophisticated language model using PyTorch's high-level APIs. We'll create a character-level language model with a recurrent neural network (RNN).\n",
    "\n",
    "First, let's load and prepare our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0520d78b595f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the names dataset\n",
    "with open(\"../../02 - Makemore/names.txt\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "# Convert to lowercase\n",
    "names = [name.lower() for name in names]\n",
    "\n",
    "# Add start and end tokens\n",
    "names_with_tokens = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "# Create vocabulary\n",
    "chars = sorted(list(set(\"\".join(names_with_tokens))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mappings between characters and indices\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_training_data(names: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Add start and end tokens\n",
    "    names = [\"<\" + name + \">\" for name in names]\n",
    "\n",
    "    # Create input-output pairs\n",
    "    xs = []  # Input characters\n",
    "    ys = []  # Target characters (next character)\n",
    "\n",
    "    for name in names:\n",
    "        for c1, c2 in zip(name, name[1:], strict=False):\n",
    "            xs.append(char_to_idx[c1])\n",
    "            ys.append(char_to_idx[c2])\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    xs = torch.tensor(xs, dtype=torch.long)\n",
    "    ys = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "xs, ys = prepare_training_data(names)\n",
    "\n",
    "print(f\"Number of training examples: {len(xs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8c9dca2e4ad9f",
   "metadata": {},
   "source": [
    "### Defining the RNN Language Model\n",
    "\n",
    "Now, let's define our RNN-based language model using PyTorch's nn module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655c764fe7cd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int) -> None:\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, hidden: torch.Tensor | None = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        # If sequence_length is 1, we're predicting one character at a time\n",
    "\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # Pass through RNN\n",
    "        output, hidden = self.rnn(\n",
    "            embedded, hidden\n",
    "        )  # output: (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(output)  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        # Initialize hidden state\n",
    "        return torch.zeros(1, batch_size, self.rnn.hidden_size)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14506599df20fec",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Let's train our RNN language model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8347a87e2331279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "seq_length = 1  # We're treating each character as a sequence of length 1\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(xs.unsqueeze(1), ys)  # Add sequence dimension\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Adjust hidden state size for the last batch which might be smaller\n",
    "        if inputs.size(0) != batch_size:\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden = model(inputs, hidden)\n",
    "\n",
    "        # Reshape output for loss calculation\n",
    "        output = output.squeeze(1)  # Remove sequence dimension\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Detach hidden state to prevent backprop through the entire sequence\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701f801680b3837",
   "metadata": {},
   "source": [
    "### Generating Names with the RNN Model\n",
    "\n",
    "Now, let's use our trained RNN model to generate new names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4d17bfa44034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_rnn(model: RNNLanguageModel, max_len: int = 20) -> str:\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Start with the start token\n",
    "        current_idx = char_to_idx[\"<\"]\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        # Store generated characters\n",
    "        chars = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Prepare input\n",
    "            x = torch.tensor([[current_idx]], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            output, hidden = model(x, hidden)\n",
    "\n",
    "            # Get probabilities\n",
    "            probs = torch.softmax(output.squeeze(), dim=0)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            current_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # Convert to character\n",
    "            current_char = idx_to_char[current_idx]\n",
    "\n",
    "            # Check if end token\n",
    "            if current_char == \">\":\n",
    "                break\n",
    "\n",
    "            # Add to result\n",
    "            chars.append(current_char)\n",
    "\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "# Generate 10 names\n",
    "print(\"Names generated using RNN model:\")\n",
    "for _ in range(10):\n",
    "    name = generate_name_rnn(model)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134dc92c9cdeead",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we've covered the essential PyTorch concepts needed for building neural networks:\n",
    "\n",
    "1. **Tensors**: PyTorch's fundamental data structure for representing and manipulating data\n",
    "2. **Autograd**: Automatic differentiation for computing gradients during backpropagation\n",
    "3. **Neural Network API**: Building models with PyTorch's nn module\n",
    "4. **Optimizers**: Using PyTorch's optimization algorithms for training neural networks\n",
    "5. **Practical Example**: Building an RNN-based language model for generating names\n",
    "\n",
    "These concepts form the foundation for building more sophisticated neural network architectures, such as those used in state-of-the-art language models like GPT.\n",
    "\n",
    "### Key Takeaways for Senior Developers\n",
    "\n",
    "- **PyTorch's Design**: PyTorch follows a \"define-by-run\" paradigm, making it more intuitive and flexible than static graph frameworks\n",
    "- **Tensor Operations**: Most NumPy operations have PyTorch equivalents, but with added benefits like GPU acceleration and automatic differentiation\n",
    "- **Neural Network Building Blocks**: PyTorch provides high-level abstractions for common neural network components\n",
    "- **Training Loop Pattern**: PyTorch's training loop pattern (forward pass, loss computation, backward pass, parameter update) is consistent across different model architectures\n",
    "- **Ecosystem Integration**: PyTorch integrates well with Python's scientific computing ecosystem (NumPy, Pandas, Matplotlib, etc.)\n",
    "\n",
    "With these fundamentals, you're now equipped to explore more advanced neural network architectures and techniques for natural language processing and other machine learning tasks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
